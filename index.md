layout: page
title: main
permalink: /main/
---

# Musika! <br/> Fast Infinite Waveform Music Generation

<!-- ![Banner](logo.png) -->
<img src="logo.png">

{% include video source="youtube" id="QBl8y2Z_i7Y" %}

**Abstract**  
Fast and user-controllable music generation could enable novel ways of composing music. However, state-of-the-art music generation systems require large amounts of data and computational resources for training, and are slow at inference. This makes them impractical for real-time interactive use. In this work, we introduce Musika, a music generation system that can be trained on hundreds of hours of music using a single consumer GPU, and that allows for much faster than real-time generation of music of arbitrary length on a consumer CPU. We achieve this by first learning a compact invertible representation of spectrogram magnitudes and phases with adversarial autoencoders, then training a Generative Adversarial Network (GAN) on this representation for a particular music domain. A latent coordinate system enables generating arbitrarily long sequences of excerpts in parallel, while a global context vector allows the music to remain stylistically coherent through time. We perform quantitative evaluations to assess the quality of the generated samples and showcase options for user control in piano and techno music generation. We release the source code and pretrained autoencoder weights, such that a GAN can be trained on a new music domain with a single GPU in a matter of hours.


## Unconditional Techno Music Generation

We train an unconditional Musika system on a dataset of 10,190 techno songs from [Jamendo](jamendo.com). We use latent vectors from universal first level and second level autoencoders, both trained to reconstruct a wide range of audio domains.  
We showcase short and long stereo samples generated by the system.

### 23 seconds samples
<br/>
<audio src="uncond_techno_mp3/0.mp3" controls ></audio>
<img src="uncond_techno_mp3/0.png">
<br/>
<audio src="uncond_techno_mp3/4.mp3" controls ></audio>
<img src="uncond_techno_mp3/4.png">
<br/>
<audio src="uncond_techno_mp3/5.mp3" controls ></audio>
<img src="uncond_techno_mp3/5.png">
<br/>
<audio src="uncond_techno_mp3/7.mp3" controls ></audio>
<img src="uncond_techno_mp3/7.png">
<br/>
<audio src="uncond_techno_mp3/8.mp3" controls ></audio>
<img src="uncond_techno_mp3/8.png">
<br/>
<audio src="uncond_techno_mp3/9.mp3" controls ></audio>
<img src="uncond_techno_mp3/9.png">
<br/>
<audio src="uncond_techno_mp3/10.mp3" controls ></audio>
<img src="uncond_techno_mp3/10.png">
<br/>
<audio src="uncond_techno_mp3/11.mp3" controls ></audio>
<img src="uncond_techno_mp3/11.png">
<br/>
<audio src="uncond_techno_mp3/12.mp3" controls ></audio>
<img src="uncond_techno_mp3/12.png">
<br/>
<audio src="uncond_techno_mp3/13.mp3" controls ></audio>
<img src="uncond_techno_mp3/13.png">

### 4 minutes samples
<br/>
<audio src="uncond_techno_mp3/long/0.mp3" controls ></audio>
<audio src="uncond_techno_mp3/long/2.mp3" controls ></audio>
<audio src="uncond_techno_mp3/long/3.mp3" controls ></audio>
<audio src="uncond_techno_mp3/long/4.mp3" controls ></audio>
<audio src="uncond_techno_mp3/long/6.mp3" controls ></audio>
<audio src="uncond_techno_mp3/long/10.mp3" controls ></audio>


## Conditional Techno Music Generation

We use the [Tempo-CNN](https://github.com/hendriks73/tempo-cnn) framework to extract tempo information from each song in our techno dataset. We then use tempo as conditioning for a latent GAN. This allows users to interact with the generation process by proposing a custom tempo before generating a sample.  
We showcase samples generated using tempo conditioning spanning from 120 to 160 bpm.

### 120 bpm
<br/>
<audio src="cond_techno_mp3/120_2.mp3" controls ></audio>
<img src="cond_techno_mp3/120_2.png">
<br/>
<audio src="cond_techno_mp3/120_3.mp3" controls ></audio>
<img src="cond_techno_mp3/120_3.png">

### 125 bpm
<br/>
<audio src="cond_techno_mp3/125_2.mp3" controls ></audio>
<img src="cond_techno_mp3/125_2.png">
<br/>
<audio src="cond_techno_mp3/125_3.mp3" controls ></audio>
<img src="cond_techno_mp3/125_3.png">

### 130 bpm
<br/>
<audio src="cond_techno_mp3/130_1.mp3" controls ></audio>
<img src="cond_techno_mp3/130_1.png">
<br/>
<audio src="cond_techno_mp3/130_2.mp3" controls ></audio>
<img src="cond_techno_mp3/130_2.png">

### 135 bpm
<br/>
<audio src="cond_techno_mp3/135_1.mp3" controls ></audio>
<img src="cond_techno_mp3/135_1.png">
<br/>
<audio src="cond_techno_mp3/135_3.mp3" controls ></audio>
<img src="cond_techno_mp3/135_3.png">

### 140 bpm
<br/>
<audio src="cond_techno_mp3/140_1.mp3" controls ></audio>
<img src="cond_techno_mp3/140_1.png">
<br/>
<audio src="cond_techno_mp3/140_3.mp3" controls ></audio>
<img src="cond_techno_mp3/140_3.png">

### 145 bpm
<br/>
<audio src="cond_techno_mp3/145_0.mp3" controls ></audio>
<img src="cond_techno_mp3/145_0.png">
<br/>
<audio src="cond_techno_mp3/145_2.mp3" controls ></audio>
<img src="cond_techno_mp3/145_2.png">

### 150 bpm
<br/>
<audio src="cond_techno_mp3/150_0.mp3" controls ></audio>
<img src="cond_techno_mp3/150_0.png">
<br/>
<audio src="cond_techno_mp3/150_3.mp3" controls ></audio>
<img src="cond_techno_mp3/150_3.png">

### 155 bpm
<br/>
<audio src="cond_techno_mp3/155_0.mp3" controls ></audio>
<img src="cond_techno_mp3/155_0.png">
<br/>
<audio src="cond_techno_mp3/155_3.mp3" controls ></audio>
<img src="cond_techno_mp3/155_3.png">

### 160 bpm
<br/>
<audio src="cond_techno_mp3/160_0.mp3" controls ></audio>
<img src="cond_techno_mp3/160_0.png">
<br/>
<audio src="cond_techno_mp3/160_1.mp3" controls ></audio>
<img src="cond_techno_mp3/160_1.png">


## Unconditional Classical Music Generation

We train an unconditional Musika system on a dataset of 1000 hours of classical music sraped from the internet. We use latent vectors from universal first level and second level autoencoders, both trained to reconstruct a wide range of audio domains.  
We showcase long stereo samples generated by the system.

### 4 minutes samples
<br/>
<audio src="classical/0.mp3" controls ></audio>
<audio src="classical/1.mp3" controls ></audio>
<audio src="classical/2.mp3" controls ></audio>
<audio src="classical/3.mp3" controls ></audio>
<audio src="classical/4.mp3" controls ></audio>
<audio src="classical/5.mp3" controls ></audio>
<audio src="classical/6.mp3" controls ></audio>
<audio src="classical/7.mp3" controls ></audio>
<audio src="classical/8.mp3" controls ></audio>
<audio src="classical/9.mp3" controls ></audio>
<audio src="classical/10.mp3" controls ></audio>
<audio src="classical/11.mp3" controls ></audio>
<audio src="classical/12.mp3" controls ></audio>
<audio src="classical/13.mp3" controls ></audio>


## Unconditional Piano Music Generation

We train an unconditional Musika system on the [MAESTRO](https://magenta.tensorflow.org/datasets/maestro) dataset, consisting in 200 hours of piano performances. To decode the generated latent vector sequence, we use a second level decoder, trained specifically to reconstruct piano music, and a universal first level decoder, trained to reconstruct a wider range of audio domains.  
Since the system is able to generate samples of arbitrary length, we showcase some short samples and some longer ones.  
We also provide short and long samples generated by [UNAGAN](https://arxiv.org/abs/2005.08526), which to the best of our knowledge is the only non-autoregressive music generation system able to generate audio of arbitrary length. While Musika generates stereo audio, UNAGAN can only generate single-channel audio.

### 23 seconds samples
<br/>
<audio src="uncond_piano_mp3/0.mp3" controls ></audio>
<img src="uncond_piano_mp3/0.png">
<br/>
<audio src="uncond_piano_mp3/1.mp3" controls ></audio>
<img src="uncond_piano_mp3/1.png">
<br/>
<audio src="uncond_piano_mp3/2.mp3" controls ></audio>
<img src="uncond_piano_mp3/2.png">
<br/>
<audio src="uncond_piano_mp3/3.mp3" controls ></audio>
<img src="uncond_piano_mp3/3.png">
<br/>
<audio src="uncond_piano_mp3/4.mp3" controls ></audio>
<img src="uncond_piano_mp3/4.png">
<br/>
<audio src="uncond_piano_mp3/5.mp3" controls ></audio>
<img src="uncond_piano_mp3/5.png">
<br/>
<audio src="uncond_piano_mp3/6.mp3" controls ></audio>
<img src="uncond_piano_mp3/6.png">
<br/>
<audio src="uncond_piano_mp3/7.mp3" controls ></audio>
<img src="uncond_piano_mp3/7.png">
<br/>
<audio src="uncond_piano_mp3/8.mp3" controls ></audio>
<img src="uncond_piano_mp3/8.png">
<br/>
<audio src="uncond_piano_mp3/9.mp3" controls ></audio>
<img src="uncond_piano_mp3/9.png">

### 4 minutes samples
<br/>
<audio src="uncond_piano_mp3/long/0.mp3" controls ></audio>
<audio src="uncond_piano_mp3/long/1.mp3" controls ></audio>
<audio src="uncond_piano_mp3/long/2.mp3" controls ></audio>
<audio src="uncond_piano_mp3/long/3.mp3" controls ></audio>
<audio src="uncond_piano_mp3/long/4.mp3" controls ></audio>
<audio src="uncond_piano_mp3/long/5.mp3" controls ></audio>

### UNAGAN samples for comparison
<br/>
<audio src="unagan_piano_23/0.mp3" controls ></audio>
<audio src="unagan_piano_23/1.mp3" controls ></audio>
<audio src="unagan_piano_23/2.mp3" controls ></audio>
<audio src="unagan_piano_23/3.mp3" controls ></audio>  
<br/>
<audio src="unagan_piano_237/0.mp3" controls ></audio>
<audio src="unagan_piano_237/1.mp3" controls ></audio>
<audio src="unagan_piano_237/2.mp3" controls ></audio>
<audio src="unagan_piano_237/3.mp3" controls ></audio>


## Conditional Piano Music Generation

We use the [MADMOM](https://github.com/CPJKU/madmom) Python library to perform onset detection for each sample in the MAESTRO dataset. We then create a note density signal and use it as conditioning for a latent GAN. This allows users to interact with the generation process by proposing a custom note density signal.  
As a demonstration, we showcase generated samples created by both feeding random signals and using constant values as note density conditioning.

### Random Note Density
<br/>
<audio src="cond_piano_mp3/0.mp3" controls ></audio>
<img src="cond_piano_mp3/0s.png">
<img src="cond_piano_mp3/0.png">
<br/>
<audio src="cond_piano_mp3/1.mp3" controls ></audio>
<img src="cond_piano_mp3/1s.png">
<img src="cond_piano_mp3/1.png">
<br/>
<audio src="cond_piano_mp3/2.mp3" controls ></audio>
<img src="cond_piano_mp3/2s.png">
<img src="cond_piano_mp3/2.png">
<br/>
<audio src="cond_piano_mp3/3.mp3" controls ></audio>
<img src="cond_piano_mp3/3s.png">
<img src="cond_piano_mp3/3.png">
<br/>
<audio src="cond_piano_mp3/4.mp3" controls ></audio>
<img src="cond_piano_mp3/4s.png">
<img src="cond_piano_mp3/4.png">
<br/>
<audio src="cond_piano_mp3/5.mp3" controls ></audio>
<img src="cond_piano_mp3/5s.png">
<img src="cond_piano_mp3/5.png">
<br/>
<audio src="cond_piano_mp3/6.mp3" controls ></audio>
<img src="cond_piano_mp3/6s.png">
<img src="cond_piano_mp3/6.png">

### Constant Note Density
<br/>
<audio src="cond_piano_mp3/7.mp3" controls ></audio>
<img src="cond_piano_mp3/7s.png">
<img src="cond_piano_mp3/7.png">
<br/>
<audio src="cond_piano_mp3/8.mp3" controls ></audio>
<img src="cond_piano_mp3/8s.png">
<img src="cond_piano_mp3/8.png">
<br/>
<audio src="cond_piano_mp3/9.mp3" controls ></audio>
<img src="cond_piano_mp3/9s.png">
<img src="cond_piano_mp3/9.png">
<br/>
<audio src="cond_piano_mp3/10.mp3" controls ></audio>
<img src="cond_piano_mp3/10s.png">
<img src="cond_piano_mp3/10.png">


<!-- ## Welcome to GitHub Pages

You can use the [editor on GitHub](https://github.com/anonymous2732/anonymous2732.github.io/edit/main/index.md) to maintain and preview the content for your website in Markdown files.

Whenever you commit to this repository, GitHub Pages will run [Jekyll](https://jekyllrb.com/) to rebuild the pages in your site, from the content in your Markdown files.

### Markdown

Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for

```markdown
Syntax highlighted code block

# Header 1
## Header 2
### Header 3

- Bulleted
- List

1. Numbered
2. List

**Bold** and _Italic_ and `Code` text

[Link](url) and ![Image](src)
```

For more details see [Basic writing and formatting syntax](https://docs.github.com/en/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax).

### Jekyll Themes

Your Pages site will use the layout and styles from the Jekyll theme you have selected in your [repository settings](https://github.com/anonymous2732/anonymous2732.github.io/settings/pages). The name of this theme is saved in the Jekyll `_config.yml` configuration file.

### Support or Contact

Having trouble with Pages? Check out our [documentation](https://docs.github.com/categories/github-pages-basics/) or [contact support](https://support.github.com/contact) and weâ€™ll help you sort it out.
 -->
